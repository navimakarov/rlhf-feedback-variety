{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff75b890-2478-468e-90a9-1ad0b4a2ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanmakarov/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "from imitation.data.types import (\n",
    "    AnyPath,\n",
    "    TrajectoryPair,\n",
    "    TrajectoryWithRew,\n",
    "    TrajectoryWithRewPair,\n",
    "    Transitions,\n",
    ")\n",
    "\n",
    "from imitation.data import rollout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25659733-9f84-490f-9a18-114cb6f1a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3177298-c251-4841-84fc-f54850d42244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyRewardLossCustom(preference_comparisons.RewardLoss):\n",
    "    \"\"\"Compute the cross entropy reward loss.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Create cross entropy reward loss.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        fragment_pairs: Sequence[TrajectoryPair],\n",
    "        preferences: np.ndarray,\n",
    "        preference_model: preference_comparisons.PreferenceModel,\n",
    "    ) -> preference_comparisons.LossAndMetrics:\n",
    "        \"\"\"Computes the loss.\n",
    "\n",
    "        Args:\n",
    "            fragment_pairs: Batch consisting of pairs of trajectory fragments.\n",
    "            preferences: The probability that the first fragment is preferred\n",
    "                over the second. Typically 0, 1 or 0.5 (tie).\n",
    "            preference_model: model to predict the preferred fragment from a pair.\n",
    "\n",
    "        Returns:\n",
    "            The cross-entropy loss between the probability predicted by the\n",
    "                reward model and the target probabilities in `preferences`. Metrics\n",
    "                are accuracy, and gt_reward_loss, if the ground truth reward is\n",
    "                available.\n",
    "        \"\"\"\n",
    "        rews_pred = th.empty(2 * len(fragment_pairs), dtype=th.float32)\n",
    "        rews_true = th.empty(2 * len(fragment_pairs), dtype=th.float32)\n",
    "\n",
    "        cnt1 = 0\n",
    "        cnt2 = 0\n",
    "        \n",
    "        for fragment in fragment_pairs:\n",
    "            frag1, frag2 = fragment\n",
    "            trans1 = rollout.flatten_trajectories([frag1])\n",
    "            trans2 = rollout.flatten_trajectories([frag2])\n",
    "            rews_pred[cnt1] = preference_model.rewards(trans1).sum()\n",
    "            cnt1 += 1\n",
    "            rews_pred[cnt1] = preference_model.rewards(trans2).sum()\n",
    "            cnt1 += 1\n",
    "\n",
    "            rews_true[cnt2] = th.tensor(frag1.rews.sum(), dtype=th.float32)\n",
    "            cnt2 += 1\n",
    "            rews_true[cnt2] = th.tensor(frag2.rews.sum(), dtype=th.float32)\n",
    "            cnt2 += 1\n",
    "\n",
    "\n",
    "        loss = th.nn.MSELoss()(rews_pred, rews_true)\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics[\"accuracy\"] = th.tensor(0)\n",
    "        metrics = {key: value.detach().cpu() for key, value in metrics.items()}\n",
    "        return preference_comparisons.LossAndMetrics(\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49b7575d-eed2-406b-ba33-0247740c91ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [30, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Collecting 60 fragments (6000 transitions)\n",
      "Requested 5700 transitions but only 0 in buffer. Sampling 5700 additional transitions.\n",
      "Sampling 300 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 30 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e390b0f275340e79e498d1fb8c60d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -296     |\n",
      "|    agent/time/fps                    | 19750    |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -296     |\n",
      "|    agent/time/fps                    | 1.98e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2.05e+03 |\n",
      "|    agent/train/approx_kl             | 0.00201  |\n",
      "|    agent/train/clip_fraction         | 0.0806   |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.691   |\n",
      "|    agent/train/explained_variance    | -1.87    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.0105   |\n",
      "|    agent/train/n_updates             | 10       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00364 |\n",
      "|    agent/train/value_loss            | 0.0451   |\n",
      "|    preferences/entropy               | 0.165    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 988      |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 779      |\n",
      "|    reward/epoch-10/train/accuracy    | 0        |\n",
      "|    reward/epoch-10/train/loss        | 160      |\n",
      "|    reward/epoch-11/train/accuracy    | 0        |\n",
      "|    reward/epoch-11/train/loss        | 133      |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 675      |\n",
      "|    reward/epoch-3/train/accuracy     | 0        |\n",
      "|    reward/epoch-3/train/loss         | 577      |\n",
      "|    reward/epoch-4/train/accuracy     | 0        |\n",
      "|    reward/epoch-4/train/loss         | 490      |\n",
      "|    reward/epoch-5/train/accuracy     | 0        |\n",
      "|    reward/epoch-5/train/loss         | 414      |\n",
      "|    reward/epoch-6/train/accuracy     | 0        |\n",
      "|    reward/epoch-6/train/loss         | 344      |\n",
      "|    reward/epoch-7/train/accuracy     | 0        |\n",
      "|    reward/epoch-7/train/loss         | 286      |\n",
      "|    reward/epoch-8/train/accuracy     | 0        |\n",
      "|    reward/epoch-8/train/loss         | 235      |\n",
      "|    reward/epoch-9/train/accuracy     | 0        |\n",
      "|    reward/epoch-9/train/loss         | 193      |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 133      |\n",
      "---------------------------------------------------\n",
      "Collecting 14 fragments (1400 transitions)\n",
      "Requested 1330 transitions but only 0 in buffer. Sampling 1330 additional transitions.\n",
      "Sampling 70 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 37 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4a28365d064147814900a93e76997c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -166         |\n",
      "|    agent/time/fps                    | 22307        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0020143995 |\n",
      "|    agent/train/clip_fraction         | 0.0806       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.691       |\n",
      "|    agent/train/explained_variance    | -1.87        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0105       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00364     |\n",
      "|    agent/train/value_loss            | 0.0451       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -166     |\n",
      "|    agent/time/fps                    | 2.23e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 4.1e+03  |\n",
      "|    agent/train/approx_kl             | 0.00306  |\n",
      "|    agent/train/clip_fraction         | 0.103    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.689   |\n",
      "|    agent/train/explained_variance    | 0.643    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00308  |\n",
      "|    agent/train/n_updates             | 20       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00391 |\n",
      "|    agent/train/value_loss            | 0.0534   |\n",
      "|    preferences/entropy               | 0.496    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 310      |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 113      |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 131      |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 131      |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 43 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db34a4fbeb2145ab9993f95616d056d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -96.9        |\n",
      "|    agent/time/fps                    | 21725        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0030591032 |\n",
      "|    agent/train/clip_fraction         | 0.103        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.689       |\n",
      "|    agent/train/explained_variance    | 0.643        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00308      |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00391     |\n",
      "|    agent/train/value_loss            | 0.0534       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -96.9    |\n",
      "|    agent/time/fps                    | 2.17e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 6.14e+03 |\n",
      "|    agent/train/approx_kl             | 0.00124  |\n",
      "|    agent/train/clip_fraction         | 0.0708   |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.686   |\n",
      "|    agent/train/explained_variance    | 0.771    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.0108   |\n",
      "|    agent/train/n_updates             | 30       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00156 |\n",
      "|    agent/train/value_loss            | 0.0543   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 203      |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 148      |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 133      |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 133      |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 49 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ba1083a3844f6c9b086a183d7e03e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -57.2        |\n",
      "|    agent/time/fps                    | 23597        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0012439466 |\n",
      "|    agent/train/clip_fraction         | 0.0708       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.686       |\n",
      "|    agent/train/explained_variance    | 0.771        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0108       |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00156     |\n",
      "|    agent/train/value_loss            | 0.0543       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -57.2    |\n",
      "|    agent/time/fps                    | 2.36e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 8.19e+03 |\n",
      "|    agent/train/approx_kl             | 0.00277  |\n",
      "|    agent/train/clip_fraction         | 0.149    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.673   |\n",
      "|    agent/train/explained_variance    | 0.631    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0093  |\n",
      "|    agent/train/n_updates             | 40       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00663 |\n",
      "|    agent/train/value_loss            | 0.0248   |\n",
      "|    preferences/entropy               | 0.593    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 120      |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 92.9     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 78.4     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 78.4     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 55 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffb4878fbf041018635165d5b97a689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -36.3       |\n",
      "|    agent/time/fps                    | 22814       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 10240       |\n",
      "|    agent/train/approx_kl             | 0.002770629 |\n",
      "|    agent/train/clip_fraction         | 0.149       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.673      |\n",
      "|    agent/train/explained_variance    | 0.631       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0093     |\n",
      "|    agent/train/n_updates             | 40          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00663    |\n",
      "|    agent/train/value_loss            | 0.0248      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -36.3    |\n",
      "|    agent/time/fps                    | 2.28e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.02e+04 |\n",
      "|    agent/train/approx_kl             | 0.0037   |\n",
      "|    agent/train/clip_fraction         | 0.196    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.673   |\n",
      "|    agent/train/explained_variance    | 0.927    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00874  |\n",
      "|    agent/train/n_updates             | 50       |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112  |\n",
      "|    agent/train/value_loss            | 0.0158   |\n",
      "|    preferences/entropy               | 0.349    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 76.3     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 64.6     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 52.4     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 52.4     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 61 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e35d50705a244d7bfd11d9bdfa86250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -15.9      |\n",
      "|    agent/time/fps                    | 22582      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 12288      |\n",
      "|    agent/train/approx_kl             | 0.00370212 |\n",
      "|    agent/train/clip_fraction         | 0.196      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.673     |\n",
      "|    agent/train/explained_variance    | 0.927      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.00874    |\n",
      "|    agent/train/n_updates             | 50         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112    |\n",
      "|    agent/train/value_loss            | 0.0158     |\n",
      "-----------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -15.9    |\n",
      "|    agent/time/fps                    | 2.26e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.23e+04 |\n",
      "|    agent/train/approx_kl             | 0.00417  |\n",
      "|    agent/train/clip_fraction         | 0.167    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.66    |\n",
      "|    agent/train/explained_variance    | 0.801    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00633 |\n",
      "|    agent/train/n_updates             | 60       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00592 |\n",
      "|    agent/train/value_loss            | 0.0123   |\n",
      "|    preferences/entropy               | 0.379    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 46.3     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 43.3     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 41.7     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 41.7     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 67 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e6c096a4734db7b5c47995c39f52eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 33.3        |\n",
      "|    agent/time/fps                    | 19802       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.004168664 |\n",
      "|    agent/train/clip_fraction         | 0.167       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.66       |\n",
      "|    agent/train/explained_variance    | 0.801       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00633    |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00592    |\n",
      "|    agent/train/value_loss            | 0.0123      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 33.3     |\n",
      "|    agent/time/fps                    | 1.98e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.43e+04 |\n",
      "|    agent/train/approx_kl             | 0.00414  |\n",
      "|    agent/train/clip_fraction         | 0.192    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.641   |\n",
      "|    agent/train/explained_variance    | 0.747    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00513  |\n",
      "|    agent/train/n_updates             | 70       |\n",
      "|    agent/train/policy_gradient_loss  | -0.0083  |\n",
      "|    agent/train/value_loss            | 0.00936  |\n",
      "|    preferences/entropy               | 0.465    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 26.9     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 30.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 28.7     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 28.7     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 73 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2ff65ddfee49138b91f7136d0b3c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 44.5         |\n",
      "|    agent/time/fps                    | 21248        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0041376296 |\n",
      "|    agent/train/clip_fraction         | 0.192        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.641       |\n",
      "|    agent/train/explained_variance    | 0.747        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00513      |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0083      |\n",
      "|    agent/train/value_loss            | 0.00936      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 44.5     |\n",
      "|    agent/time/fps                    | 2.12e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.64e+04 |\n",
      "|    agent/train/approx_kl             | 0.00412  |\n",
      "|    agent/train/clip_fraction         | 0.223    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.651   |\n",
      "|    agent/train/explained_variance    | 0.545    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.017   |\n",
      "|    agent/train/n_updates             | 80       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00814 |\n",
      "|    agent/train/value_loss            | 0.00493  |\n",
      "|    preferences/entropy               | 0.199    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 32.1     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 30.8     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 28.8     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 28.8     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 79 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f178ecf17bc436c9a0e2067559e0a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 36.3         |\n",
      "|    agent/time/fps                    | 23582        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0041196425 |\n",
      "|    agent/train/clip_fraction         | 0.223        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.651       |\n",
      "|    agent/train/explained_variance    | 0.545        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.017       |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00814     |\n",
      "|    agent/train/value_loss            | 0.00493      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 36.3     |\n",
      "|    agent/time/fps                    | 2.36e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.84e+04 |\n",
      "|    agent/train/approx_kl             | 0.00619  |\n",
      "|    agent/train/clip_fraction         | 0.332    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.63    |\n",
      "|    agent/train/explained_variance    | 0.842    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0415  |\n",
      "|    agent/train/n_updates             | 90       |\n",
      "|    agent/train/policy_gradient_loss  | -0.0159  |\n",
      "|    agent/train/value_loss            | 0.00343  |\n",
      "|    preferences/entropy               | 0.368    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 25.4     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 25.1     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 23.6     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 23.6     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 85 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ce95716ffa4e3e8fe173c0030edd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 27.2         |\n",
      "|    agent/time/fps                    | 21815        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 20480        |\n",
      "|    agent/train/approx_kl             | 0.0061916243 |\n",
      "|    agent/train/clip_fraction         | 0.332        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.63        |\n",
      "|    agent/train/explained_variance    | 0.842        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0415      |\n",
      "|    agent/train/n_updates             | 90           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0159      |\n",
      "|    agent/train/value_loss            | 0.00343      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 27.2     |\n",
      "|    agent/time/fps                    | 2.18e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2.05e+04 |\n",
      "|    agent/train/approx_kl             | 0.00664  |\n",
      "|    agent/train/clip_fraction         | 0.271    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.613   |\n",
      "|    agent/train/explained_variance    | 0.913    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0115  |\n",
      "|    agent/train/n_updates             | 100      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113  |\n",
      "|    agent/train/value_loss            | 0.00357  |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 23.6     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 23.4     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 22.8     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 22.8     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 91 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3c8d40690740bfa28cf83f8c3830da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.8        |\n",
      "|    agent/time/fps                    | 21332       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.006640197 |\n",
      "|    agent/train/clip_fraction         | 0.271       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.613      |\n",
      "|    agent/train/explained_variance    | 0.913       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0115     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113     |\n",
      "|    agent/train/value_loss            | 0.00357     |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.8     |\n",
      "|    agent/time/fps                    | 2.13e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2.25e+04 |\n",
      "|    agent/train/approx_kl             | 0.00734  |\n",
      "|    agent/train/clip_fraction         | 0.238    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.588   |\n",
      "|    agent/train/explained_variance    | 0.927    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00948  |\n",
      "|    agent/train/n_updates             | 110      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00806 |\n",
      "|    agent/train/value_loss            | 0.00464  |\n",
      "|    preferences/entropy               | 0.501    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 21.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 22.4     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 22.3     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 22.3     |\n",
      "---------------------------------------------------\n",
      "Collecting 12 fragments (1200 transitions)\n",
      "Requested 1140 transitions but only 0 in buffer. Sampling 1140 additional transitions.\n",
      "Sampling 60 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 97 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e5db1da7974b5289565eb8c0e8f418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17.2        |\n",
      "|    agent/time/fps                    | 4921        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 24576       |\n",
      "|    agent/train/approx_kl             | 0.007341261 |\n",
      "|    agent/train/clip_fraction         | 0.238       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.588      |\n",
      "|    agent/train/explained_variance    | 0.927       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00948     |\n",
      "|    agent/train/n_updates             | 110         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00806    |\n",
      "|    agent/train/value_loss            | 0.00464     |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17.2     |\n",
      "|    agent/time/fps                    | 4.92e+03 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2.46e+04 |\n",
      "|    agent/train/approx_kl             | 0.00446  |\n",
      "|    agent/train/clip_fraction         | 0.173    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.539   |\n",
      "|    agent/train/explained_variance    | 0.946    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0113  |\n",
      "|    agent/train/n_updates             | 120      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0047  |\n",
      "|    agent/train/value_loss            | 0.0039   |\n",
      "|    preferences/entropy               | 0.263    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 21       |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 22.7     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 21       |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 21       |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 102 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793db2c9992d42428b0834bebbbd774d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.7         |\n",
      "|    agent/time/fps                    | 21233        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 26624        |\n",
      "|    agent/train/approx_kl             | 0.0044561257 |\n",
      "|    agent/train/clip_fraction         | 0.173        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.539       |\n",
      "|    agent/train/explained_variance    | 0.946        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0113      |\n",
      "|    agent/train/n_updates             | 120          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0047      |\n",
      "|    agent/train/value_loss            | 0.0039       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.7     |\n",
      "|    agent/time/fps                    | 2.12e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2.66e+04 |\n",
      "|    agent/train/approx_kl             | 0.00395  |\n",
      "|    agent/train/clip_fraction         | 0.223    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.528   |\n",
      "|    agent/train/explained_variance    | 0.962    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0218  |\n",
      "|    agent/train/n_updates             | 130      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00952 |\n",
      "|    agent/train/value_loss            | 0.00674  |\n",
      "|    preferences/entropy               | 0.556    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 22.6     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 21.8     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 22.1     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 22.1     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 107 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b867b92576804ae6839e0060c1992cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.1         |\n",
      "|    agent/time/fps                    | 23922        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 28672        |\n",
      "|    agent/train/approx_kl             | 0.0039471877 |\n",
      "|    agent/train/clip_fraction         | 0.223        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.528       |\n",
      "|    agent/train/explained_variance    | 0.962        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0218      |\n",
      "|    agent/train/n_updates             | 130          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00952     |\n",
      "|    agent/train/value_loss            | 0.00674      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.1     |\n",
      "|    agent/time/fps                    | 2.39e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2.87e+04 |\n",
      "|    agent/train/approx_kl             | 0.00335  |\n",
      "|    agent/train/clip_fraction         | 0.164    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.49    |\n",
      "|    agent/train/explained_variance    | 0.944    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0181  |\n",
      "|    agent/train/n_updates             | 140      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00522 |\n",
      "|    agent/train/value_loss            | 0.0104   |\n",
      "|    preferences/entropy               | 0.593    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 22.1     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 23.3     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 21       |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 21       |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 112 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8c43f29a5b4220b753f566f2274cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.2         |\n",
      "|    agent/time/fps                    | 24150        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 30720        |\n",
      "|    agent/train/approx_kl             | 0.0033466732 |\n",
      "|    agent/train/clip_fraction         | 0.164        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.49        |\n",
      "|    agent/train/explained_variance    | 0.944        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0181      |\n",
      "|    agent/train/n_updates             | 140          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00522     |\n",
      "|    agent/train/value_loss            | 0.0104       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.2     |\n",
      "|    agent/time/fps                    | 2.42e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 3.07e+04 |\n",
      "|    agent/train/approx_kl             | 0.00352  |\n",
      "|    agent/train/clip_fraction         | 0.164    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.444   |\n",
      "|    agent/train/explained_variance    | 0.929    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00723 |\n",
      "|    agent/train/n_updates             | 150      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00695 |\n",
      "|    agent/train/value_loss            | 0.00624  |\n",
      "|    preferences/entropy               | 0.555    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 22.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 21.7     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 24.3     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 24.3     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 117 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911e4be76d7049f79bffc325e5f15263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.3         |\n",
      "|    agent/time/fps                    | 23708        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 32768        |\n",
      "|    agent/train/approx_kl             | 0.0035190007 |\n",
      "|    agent/train/clip_fraction         | 0.164        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.444       |\n",
      "|    agent/train/explained_variance    | 0.929        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00723     |\n",
      "|    agent/train/n_updates             | 150          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00695     |\n",
      "|    agent/train/value_loss            | 0.00624      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.3     |\n",
      "|    agent/time/fps                    | 2.37e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 3.28e+04 |\n",
      "|    agent/train/approx_kl             | 0.00462  |\n",
      "|    agent/train/clip_fraction         | 0.16     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.428   |\n",
      "|    agent/train/explained_variance    | 0.906    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00906 |\n",
      "|    agent/train/n_updates             | 160      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00532 |\n",
      "|    agent/train/value_loss            | 0.00396  |\n",
      "|    preferences/entropy               | 0.472    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 20.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 24.7     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 20.9     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 20.9     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 122 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf363801d324db0a776bb9a6a10c1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.3         |\n",
      "|    agent/time/fps                    | 24080        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 34816        |\n",
      "|    agent/train/approx_kl             | 0.0046203807 |\n",
      "|    agent/train/clip_fraction         | 0.16         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.428       |\n",
      "|    agent/train/explained_variance    | 0.906        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00906     |\n",
      "|    agent/train/n_updates             | 160          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00532     |\n",
      "|    agent/train/value_loss            | 0.00396      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.3     |\n",
      "|    agent/time/fps                    | 2.41e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 3.48e+04 |\n",
      "|    agent/train/approx_kl             | 0.00355  |\n",
      "|    agent/train/clip_fraction         | 0.147    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.423   |\n",
      "|    agent/train/explained_variance    | 0.931    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.0114   |\n",
      "|    agent/train/n_updates             | 170      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00379 |\n",
      "|    agent/train/value_loss            | 0.00447  |\n",
      "|    preferences/entropy               | 0.309    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 20.9     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 20.4     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 19.9     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 19.9     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 127 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6c7deadfe54385bf4b030c042a9f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.4        |\n",
      "|    agent/time/fps                    | 22085       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.003545674 |\n",
      "|    agent/train/clip_fraction         | 0.147       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.423      |\n",
      "|    agent/train/explained_variance    | 0.931       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0114      |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00379    |\n",
      "|    agent/train/value_loss            | 0.00447     |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.4     |\n",
      "|    agent/time/fps                    | 2.21e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 3.69e+04 |\n",
      "|    agent/train/approx_kl             | 0.00422  |\n",
      "|    agent/train/clip_fraction         | 0.175    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.409   |\n",
      "|    agent/train/explained_variance    | 0.93     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0166  |\n",
      "|    agent/train/n_updates             | 180      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00635 |\n",
      "|    agent/train/value_loss            | 0.00543  |\n",
      "|    preferences/entropy               | 0.383    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 19.5     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 19.3     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 19.1     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 19.1     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 132 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4d89143d6b4086b4d3217be94df53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.5         |\n",
      "|    agent/time/fps                    | 23533        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 38912        |\n",
      "|    agent/train/approx_kl             | 0.0042150137 |\n",
      "|    agent/train/clip_fraction         | 0.175        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.409       |\n",
      "|    agent/train/explained_variance    | 0.93         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0166      |\n",
      "|    agent/train/n_updates             | 180          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00635     |\n",
      "|    agent/train/value_loss            | 0.00543      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.5     |\n",
      "|    agent/time/fps                    | 2.35e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 3.89e+04 |\n",
      "|    agent/train/approx_kl             | 0.00633  |\n",
      "|    agent/train/clip_fraction         | 0.181    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.442   |\n",
      "|    agent/train/explained_variance    | 0.95     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.015   |\n",
      "|    agent/train/n_updates             | 190      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00803 |\n",
      "|    agent/train/value_loss            | 0.00553  |\n",
      "|    preferences/entropy               | 0.321    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 16.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 17.5     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 16.7     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 16.7     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 137 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e753aff6704b828b0a32a668acfdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.1         |\n",
      "|    agent/time/fps                    | 23480        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 40960        |\n",
      "|    agent/train/approx_kl             | 0.0063265287 |\n",
      "|    agent/train/clip_fraction         | 0.181        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.442       |\n",
      "|    agent/train/explained_variance    | 0.95         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.015       |\n",
      "|    agent/train/n_updates             | 190          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00803     |\n",
      "|    agent/train/value_loss            | 0.00553      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.1     |\n",
      "|    agent/time/fps                    | 2.35e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 4.1e+04  |\n",
      "|    agent/train/approx_kl             | 0.00353  |\n",
      "|    agent/train/clip_fraction         | 0.207    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.437   |\n",
      "|    agent/train/explained_variance    | 0.959    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0206  |\n",
      "|    agent/train/n_updates             | 200      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0082  |\n",
      "|    agent/train/value_loss            | 0.00487  |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 17.4     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 18.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 18       |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 18       |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 142 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e665a8eed9742458f03a985382fc8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.3        |\n",
      "|    agent/time/fps                    | 22851       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.003530215 |\n",
      "|    agent/train/clip_fraction         | 0.207       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.437      |\n",
      "|    agent/train/explained_variance    | 0.959       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0206     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0082     |\n",
      "|    agent/train/value_loss            | 0.00487     |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.3     |\n",
      "|    agent/time/fps                    | 2.29e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 4.3e+04  |\n",
      "|    agent/train/approx_kl             | 0.00623  |\n",
      "|    agent/train/clip_fraction         | 0.239    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.442   |\n",
      "|    agent/train/explained_variance    | 0.962    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0199  |\n",
      "|    agent/train/n_updates             | 210      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00992 |\n",
      "|    agent/train/value_loss            | 0.00548  |\n",
      "|    preferences/entropy               | 0.571    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 17.8     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 17.1     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 17       |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 17       |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 147 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ec08d77bc9499088536d32ad23b26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.2         |\n",
      "|    agent/time/fps                    | 22142        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 45056        |\n",
      "|    agent/train/approx_kl             | 0.0062283296 |\n",
      "|    agent/train/clip_fraction         | 0.239        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.442       |\n",
      "|    agent/train/explained_variance    | 0.962        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0199      |\n",
      "|    agent/train/n_updates             | 210          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00992     |\n",
      "|    agent/train/value_loss            | 0.00548      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.2     |\n",
      "|    agent/time/fps                    | 2.21e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 4.51e+04 |\n",
      "|    agent/train/approx_kl             | 0.00386  |\n",
      "|    agent/train/clip_fraction         | 0.223    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.433   |\n",
      "|    agent/train/explained_variance    | 0.951    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00254  |\n",
      "|    agent/train/n_updates             | 220      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00728 |\n",
      "|    agent/train/value_loss            | 0.00519  |\n",
      "|    preferences/entropy               | 0.425    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 16.3     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 16.6     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 16.4     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 16.4     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 152 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978989cc97f444d29d45436f4ef66318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.7         |\n",
      "|    agent/time/fps                    | 21255        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 47104        |\n",
      "|    agent/train/approx_kl             | 0.0038637496 |\n",
      "|    agent/train/clip_fraction         | 0.223        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.433       |\n",
      "|    agent/train/explained_variance    | 0.951        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00254      |\n",
      "|    agent/train/n_updates             | 220          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00728     |\n",
      "|    agent/train/value_loss            | 0.00519      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.7     |\n",
      "|    agent/time/fps                    | 2.13e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 4.71e+04 |\n",
      "|    agent/train/approx_kl             | 0.00452  |\n",
      "|    agent/train/clip_fraction         | 0.221    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.429   |\n",
      "|    agent/train/explained_variance    | 0.95     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0265  |\n",
      "|    agent/train/n_updates             | 230      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00886 |\n",
      "|    agent/train/value_loss            | 0.00513  |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 17       |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 17       |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 16.1     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 16.1     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 157 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8e5c6972034392a20d1146d3756bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.9         |\n",
      "|    agent/time/fps                    | 21302        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 49152        |\n",
      "|    agent/train/approx_kl             | 0.0045190607 |\n",
      "|    agent/train/clip_fraction         | 0.221        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.429       |\n",
      "|    agent/train/explained_variance    | 0.95         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0265      |\n",
      "|    agent/train/n_updates             | 230          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00886     |\n",
      "|    agent/train/value_loss            | 0.00513      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.9     |\n",
      "|    agent/time/fps                    | 2.13e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 4.92e+04 |\n",
      "|    agent/train/approx_kl             | 0.00258  |\n",
      "|    agent/train/clip_fraction         | 0.158    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.399   |\n",
      "|    agent/train/explained_variance    | 0.926    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.012   |\n",
      "|    agent/train/n_updates             | 240      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00433 |\n",
      "|    agent/train/value_loss            | 0.00398  |\n",
      "|    preferences/entropy               | 0.532    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 16.2     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 16.1     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 16.3     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 16.3     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 162 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c4543a17424fa0805c69ce9506311b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.5         |\n",
      "|    agent/time/fps                    | 15175        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 51200        |\n",
      "|    agent/train/approx_kl             | 0.0025752438 |\n",
      "|    agent/train/clip_fraction         | 0.158        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.399       |\n",
      "|    agent/train/explained_variance    | 0.926        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.012       |\n",
      "|    agent/train/n_updates             | 240          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00433     |\n",
      "|    agent/train/value_loss            | 0.00398      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.5     |\n",
      "|    agent/time/fps                    | 1.52e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 5.12e+04 |\n",
      "|    agent/train/approx_kl             | 0.00475  |\n",
      "|    agent/train/clip_fraction         | 0.19     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.412   |\n",
      "|    agent/train/explained_variance    | 0.933    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00914 |\n",
      "|    agent/train/n_updates             | 250      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0053  |\n",
      "|    agent/train/value_loss            | 0.00367  |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 13.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 18.3     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 15.4     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 15.4     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 167 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1cc17e5b2640ce8b9ddc7586153976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.1        |\n",
      "|    agent/time/fps                    | 23316       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.004750454 |\n",
      "|    agent/train/clip_fraction         | 0.19        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.412      |\n",
      "|    agent/train/explained_variance    | 0.933       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00914    |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0053     |\n",
      "|    agent/train/value_loss            | 0.00367     |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.1     |\n",
      "|    agent/time/fps                    | 2.33e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 5.32e+04 |\n",
      "|    agent/train/approx_kl             | 0.00151  |\n",
      "|    agent/train/clip_fraction         | 0.105    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.425   |\n",
      "|    agent/train/explained_variance    | 0.949    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0157  |\n",
      "|    agent/train/n_updates             | 260      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00158 |\n",
      "|    agent/train/value_loss            | 0.00348  |\n",
      "|    preferences/entropy               | 0.54     |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 13.8     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 14.1     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 15.4     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 15.4     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 172 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3a68e269c4454091204b868d840e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.6        |\n",
      "|    agent/time/fps                    | 13227       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.001510687 |\n",
      "|    agent/train/clip_fraction         | 0.105       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.425      |\n",
      "|    agent/train/explained_variance    | 0.949       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0157     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00158    |\n",
      "|    agent/train/value_loss            | 0.00348     |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.6     |\n",
      "|    agent/time/fps                    | 1.32e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 5.53e+04 |\n",
      "|    agent/train/approx_kl             | 0.00234  |\n",
      "|    agent/train/clip_fraction         | 0.11     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.413   |\n",
      "|    agent/train/explained_variance    | 0.952    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0111  |\n",
      "|    agent/train/n_updates             | 270      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0019  |\n",
      "|    agent/train/value_loss            | 0.00647  |\n",
      "|    preferences/entropy               | 0.212    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 14.6     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 13.9     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 13.6     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 13.6     |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (1000 transitions)\n",
      "Requested 950 transitions but only 0 in buffer. Sampling 950 additional transitions.\n",
      "Sampling 50 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 177 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2a406484a348bdba80a238bb5d5b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.6         |\n",
      "|    agent/time/fps                    | 22597        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 57344        |\n",
      "|    agent/train/approx_kl             | 0.0023433478 |\n",
      "|    agent/train/clip_fraction         | 0.11         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.413       |\n",
      "|    agent/train/explained_variance    | 0.952        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0111      |\n",
      "|    agent/train/n_updates             | 270          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0019      |\n",
      "|    agent/train/value_loss            | 0.00647      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.6     |\n",
      "|    agent/time/fps                    | 2.26e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 5.73e+04 |\n",
      "|    agent/train/approx_kl             | 0.00235  |\n",
      "|    agent/train/clip_fraction         | 0.11     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.413   |\n",
      "|    agent/train/explained_variance    | 0.958    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00304  |\n",
      "|    agent/train/n_updates             | 280      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00125 |\n",
      "|    agent/train/value_loss            | 0.00457  |\n",
      "|    preferences/entropy               | 0.32     |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 13.9     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 16.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 13.7     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 13.7     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 181 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b0b47d9cc3426fad5b6dcf75df8abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.3         |\n",
      "|    agent/time/fps                    | 23569        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 59392        |\n",
      "|    agent/train/approx_kl             | 0.0023488472 |\n",
      "|    agent/train/clip_fraction         | 0.11         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.413       |\n",
      "|    agent/train/explained_variance    | 0.958        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00304      |\n",
      "|    agent/train/n_updates             | 280          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00125     |\n",
      "|    agent/train/value_loss            | 0.00457      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.3     |\n",
      "|    agent/time/fps                    | 2.36e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 5.94e+04 |\n",
      "|    agent/train/approx_kl             | 0.00344  |\n",
      "|    agent/train/clip_fraction         | 0.166    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.422   |\n",
      "|    agent/train/explained_variance    | 0.95     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.017   |\n",
      "|    agent/train/n_updates             | 290      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00352 |\n",
      "|    agent/train/value_loss            | 0.00722  |\n",
      "|    preferences/entropy               | 0.0106   |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 13.5     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 13.5     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 13.6     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 13.6     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 185 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12ee121aaf54d96ab524550f18afd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.6         |\n",
      "|    agent/time/fps                    | 23318        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 61440        |\n",
      "|    agent/train/approx_kl             | 0.0034422912 |\n",
      "|    agent/train/clip_fraction         | 0.166        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.422       |\n",
      "|    agent/train/explained_variance    | 0.95         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.017       |\n",
      "|    agent/train/n_updates             | 290          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00352     |\n",
      "|    agent/train/value_loss            | 0.00722      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.6     |\n",
      "|    agent/time/fps                    | 2.33e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 6.14e+04 |\n",
      "|    agent/train/approx_kl             | 0.00511  |\n",
      "|    agent/train/clip_fraction         | 0.173    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.453   |\n",
      "|    agent/train/explained_variance    | 0.869    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0154  |\n",
      "|    agent/train/n_updates             | 300      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00284 |\n",
      "|    agent/train/value_loss            | 0.0115   |\n",
      "|    preferences/entropy               | 0.522    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 13.4     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 13.6     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 13.3     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 13.3     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 189 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59222a5ad8144de6a2bd1601dc50e665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.8         |\n",
      "|    agent/time/fps                    | 22849        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 63488        |\n",
      "|    agent/train/approx_kl             | 0.0051054745 |\n",
      "|    agent/train/clip_fraction         | 0.173        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.453       |\n",
      "|    agent/train/explained_variance    | 0.869        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0154      |\n",
      "|    agent/train/n_updates             | 300          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00284     |\n",
      "|    agent/train/value_loss            | 0.0115       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.8     |\n",
      "|    agent/time/fps                    | 2.28e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 6.35e+04 |\n",
      "|    agent/train/approx_kl             | 0.00294  |\n",
      "|    agent/train/clip_fraction         | 0.154    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.486   |\n",
      "|    agent/train/explained_variance    | 0.775    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00445  |\n",
      "|    agent/train/n_updates             | 310      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00257 |\n",
      "|    agent/train/value_loss            | 0.0119   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 13.2     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 13.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 13.2     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 13.2     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 193 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7e9377bdba4eecacd60401e6145a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.1       |\n",
      "|    agent/time/fps                    | 19150      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 65536      |\n",
      "|    agent/train/approx_kl             | 0.00293953 |\n",
      "|    agent/train/clip_fraction         | 0.154      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.486     |\n",
      "|    agent/train/explained_variance    | 0.775      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.00445    |\n",
      "|    agent/train/n_updates             | 310        |\n",
      "|    agent/train/policy_gradient_loss  | -0.00257   |\n",
      "|    agent/train/value_loss            | 0.0119     |\n",
      "-----------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.1     |\n",
      "|    agent/time/fps                    | 1.92e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 6.55e+04 |\n",
      "|    agent/train/approx_kl             | 0.00431  |\n",
      "|    agent/train/clip_fraction         | 0.181    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.487   |\n",
      "|    agent/train/explained_variance    | 0.917    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.0348   |\n",
      "|    agent/train/n_updates             | 320      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00304 |\n",
      "|    agent/train/value_loss            | 0.00707  |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.8     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 11.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 11.8     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 11.8     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 197 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cda487e40948eda3fa86a47c19a237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.6         |\n",
      "|    agent/time/fps                    | 21628        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 67584        |\n",
      "|    agent/train/approx_kl             | 0.0043083206 |\n",
      "|    agent/train/clip_fraction         | 0.181        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.487       |\n",
      "|    agent/train/explained_variance    | 0.917        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0348       |\n",
      "|    agent/train/n_updates             | 320          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00304     |\n",
      "|    agent/train/value_loss            | 0.00707      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.6     |\n",
      "|    agent/time/fps                    | 2.16e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 6.76e+04 |\n",
      "|    agent/train/approx_kl             | 0.004    |\n",
      "|    agent/train/clip_fraction         | 0.185    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.517   |\n",
      "|    agent/train/explained_variance    | 0.871    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.0175   |\n",
      "|    agent/train/n_updates             | 330      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00456 |\n",
      "|    agent/train/value_loss            | 0.011    |\n",
      "|    preferences/entropy               | 0.235    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 20.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 11.8     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 11.7     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 11.7     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 201 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2945f90a41945728420de39cca0d611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17.5        |\n",
      "|    agent/time/fps                    | 21696       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 69632       |\n",
      "|    agent/train/approx_kl             | 0.004002239 |\n",
      "|    agent/train/clip_fraction         | 0.185       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.517      |\n",
      "|    agent/train/explained_variance    | 0.871       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0175      |\n",
      "|    agent/train/n_updates             | 330         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00456    |\n",
      "|    agent/train/value_loss            | 0.011       |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17.5     |\n",
      "|    agent/time/fps                    | 2.17e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 6.96e+04 |\n",
      "|    agent/train/approx_kl             | 0.00434  |\n",
      "|    agent/train/clip_fraction         | 0.203    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.523   |\n",
      "|    agent/train/explained_variance    | 0.927    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0159  |\n",
      "|    agent/train/n_updates             | 340      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00812 |\n",
      "|    agent/train/value_loss            | 0.00767  |\n",
      "|    preferences/entropy               | 0.216    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 11.3     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 12.2     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 12.2     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 205 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bdf9ec7e7748d594ccec4c191fc298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.6         |\n",
      "|    agent/time/fps                    | 23871        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 71680        |\n",
      "|    agent/train/approx_kl             | 0.0043434156 |\n",
      "|    agent/train/clip_fraction         | 0.203        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.523       |\n",
      "|    agent/train/explained_variance    | 0.927        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0159      |\n",
      "|    agent/train/n_updates             | 340          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00812     |\n",
      "|    agent/train/value_loss            | 0.00767      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.6     |\n",
      "|    agent/time/fps                    | 2.39e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 7.17e+04 |\n",
      "|    agent/train/approx_kl             | 0.00489  |\n",
      "|    agent/train/clip_fraction         | 0.25     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.528   |\n",
      "|    agent/train/explained_variance    | 0.95     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0112  |\n",
      "|    agent/train/n_updates             | 350      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109  |\n",
      "|    agent/train/value_loss            | 0.0112   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.6     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 15.7     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.8     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.8     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 209 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293734b831e944e691775a386deb3cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.8         |\n",
      "|    agent/time/fps                    | 24368        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 73728        |\n",
      "|    agent/train/approx_kl             | 0.0048871664 |\n",
      "|    agent/train/clip_fraction         | 0.25         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.528       |\n",
      "|    agent/train/explained_variance    | 0.95         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0112      |\n",
      "|    agent/train/n_updates             | 350          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109      |\n",
      "|    agent/train/value_loss            | 0.0112       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.8     |\n",
      "|    agent/time/fps                    | 2.44e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 7.37e+04 |\n",
      "|    agent/train/approx_kl             | 0.00402  |\n",
      "|    agent/train/clip_fraction         | 0.242    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.579   |\n",
      "|    agent/train/explained_variance    | 0.901    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0195  |\n",
      "|    agent/train/n_updates             | 360      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00735 |\n",
      "|    agent/train/value_loss            | 0.0154   |\n",
      "|    preferences/entropy               | 0.417    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.1     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 12.3     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 12.5     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 12.5     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 213 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ee280f20a34f6db90d1257602198be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.5         |\n",
      "|    agent/time/fps                    | 24000        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 75776        |\n",
      "|    agent/train/approx_kl             | 0.0040233885 |\n",
      "|    agent/train/clip_fraction         | 0.242        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.579       |\n",
      "|    agent/train/explained_variance    | 0.901        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0195      |\n",
      "|    agent/train/n_updates             | 360          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00735     |\n",
      "|    agent/train/value_loss            | 0.0154       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.5     |\n",
      "|    agent/time/fps                    | 2.4e+04  |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 7.58e+04 |\n",
      "|    agent/train/approx_kl             | 0.00479  |\n",
      "|    agent/train/clip_fraction         | 0.274    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.574   |\n",
      "|    agent/train/explained_variance    | 0.947    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00545  |\n",
      "|    agent/train/n_updates             | 370      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112  |\n",
      "|    agent/train/value_loss            | 0.0109   |\n",
      "|    preferences/entropy               | 0.52     |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.7     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 11.8     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 11.7     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 11.7     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 217 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e409e26adea54eb0b93a6bf3759d4acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.1         |\n",
      "|    agent/time/fps                    | 23790        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 77824        |\n",
      "|    agent/train/approx_kl             | 0.0047922824 |\n",
      "|    agent/train/clip_fraction         | 0.274        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.574       |\n",
      "|    agent/train/explained_variance    | 0.947        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00545      |\n",
      "|    agent/train/n_updates             | 370          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112      |\n",
      "|    agent/train/value_loss            | 0.0109       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.1     |\n",
      "|    agent/time/fps                    | 2.38e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 7.78e+04 |\n",
      "|    agent/train/approx_kl             | 0.00379  |\n",
      "|    agent/train/clip_fraction         | 0.254    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.583   |\n",
      "|    agent/train/explained_variance    | 0.921    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00782 |\n",
      "|    agent/train/n_updates             | 380      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00708 |\n",
      "|    agent/train/value_loss            | 0.0149   |\n",
      "|    preferences/entropy               | 0.396    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.8     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 11.6     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 11.5     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 11.5     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 221 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaaddf9f7a042c6a9a2fbf8ffb7ff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 14.9         |\n",
      "|    agent/time/fps                    | 23738        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 79872        |\n",
      "|    agent/train/approx_kl             | 0.0037874058 |\n",
      "|    agent/train/clip_fraction         | 0.254        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.583       |\n",
      "|    agent/train/explained_variance    | 0.921        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00782     |\n",
      "|    agent/train/n_updates             | 380          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00708     |\n",
      "|    agent/train/value_loss            | 0.0149       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 14.9     |\n",
      "|    agent/time/fps                    | 2.37e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 7.99e+04 |\n",
      "|    agent/train/approx_kl             | 0.00461  |\n",
      "|    agent/train/clip_fraction         | 0.28     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.581   |\n",
      "|    agent/train/explained_variance    | 0.953    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0371  |\n",
      "|    agent/train/n_updates             | 390      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00931 |\n",
      "|    agent/train/value_loss            | 0.0133   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.4     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 11.5     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 11.3     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 11.3     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 225 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a74ba581cf450cb684b1622906d34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 14.9        |\n",
      "|    agent/time/fps                    | 23950       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 81920       |\n",
      "|    agent/train/approx_kl             | 0.004605065 |\n",
      "|    agent/train/clip_fraction         | 0.28        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.581      |\n",
      "|    agent/train/explained_variance    | 0.953       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0371     |\n",
      "|    agent/train/n_updates             | 390         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00931    |\n",
      "|    agent/train/value_loss            | 0.0133      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 14.9     |\n",
      "|    agent/time/fps                    | 2.4e+04  |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 8.19e+04 |\n",
      "|    agent/train/approx_kl             | 0.00408  |\n",
      "|    agent/train/clip_fraction         | 0.241    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.575   |\n",
      "|    agent/train/explained_variance    | 0.948    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.014    |\n",
      "|    agent/train/n_updates             | 400      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00966 |\n",
      "|    agent/train/value_loss            | 0.0116   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 11.1     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10       |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.2     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.2     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 229 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0790399d91d543d09e0b360d70dec4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.6         |\n",
      "|    agent/time/fps                    | 23353        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 83968        |\n",
      "|    agent/train/approx_kl             | 0.0040751603 |\n",
      "|    agent/train/clip_fraction         | 0.241        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.575       |\n",
      "|    agent/train/explained_variance    | 0.948        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.014        |\n",
      "|    agent/train/n_updates             | 400          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00966     |\n",
      "|    agent/train/value_loss            | 0.0116       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.6     |\n",
      "|    agent/time/fps                    | 2.34e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 8.4e+04  |\n",
      "|    agent/train/approx_kl             | 0.00579  |\n",
      "|    agent/train/clip_fraction         | 0.288    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.553   |\n",
      "|    agent/train/explained_variance    | 0.941    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0232  |\n",
      "|    agent/train/n_updates             | 410      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0104  |\n",
      "|    agent/train/value_loss            | 0.0124   |\n",
      "|    preferences/entropy               | 0.52     |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.95     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 11.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.4     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.4     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 233 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1d1897702148ef8e409ef7d09ef972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16          |\n",
      "|    agent/time/fps                    | 24061       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 86016       |\n",
      "|    agent/train/approx_kl             | 0.005794633 |\n",
      "|    agent/train/clip_fraction         | 0.288       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.553      |\n",
      "|    agent/train/explained_variance    | 0.941       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0232     |\n",
      "|    agent/train/n_updates             | 410         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0104     |\n",
      "|    agent/train/value_loss            | 0.0124      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16       |\n",
      "|    agent/time/fps                    | 2.41e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 8.6e+04  |\n",
      "|    agent/train/approx_kl             | 0.00626  |\n",
      "|    agent/train/clip_fraction         | 0.269    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.542   |\n",
      "|    agent/train/explained_variance    | 0.941    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0134  |\n",
      "|    agent/train/n_updates             | 420      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00923 |\n",
      "|    agent/train/value_loss            | 0.0131   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 13.6     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10.9     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.3     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.3     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 237 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fec63dd99a4bca9635ce0d176b47c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.4         |\n",
      "|    agent/time/fps                    | 23987        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 88064        |\n",
      "|    agent/train/approx_kl             | 0.0062578293 |\n",
      "|    agent/train/clip_fraction         | 0.269        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.542       |\n",
      "|    agent/train/explained_variance    | 0.941        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0134      |\n",
      "|    agent/train/n_updates             | 420          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00923     |\n",
      "|    agent/train/value_loss            | 0.0131       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.4     |\n",
      "|    agent/time/fps                    | 2.4e+04  |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 8.81e+04 |\n",
      "|    agent/train/approx_kl             | 0.0071   |\n",
      "|    agent/train/clip_fraction         | 0.26     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.496   |\n",
      "|    agent/train/explained_variance    | 0.952    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0155  |\n",
      "|    agent/train/n_updates             | 430      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123  |\n",
      "|    agent/train/value_loss            | 0.0135   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 10.2     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10.3     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.3     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.3     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 241 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572db32f3ae143f89a8b87f34507b260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.9         |\n",
      "|    agent/time/fps                    | 24021        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 90112        |\n",
      "|    agent/train/approx_kl             | 0.0071030343 |\n",
      "|    agent/train/clip_fraction         | 0.26         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.496       |\n",
      "|    agent/train/explained_variance    | 0.952        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0155      |\n",
      "|    agent/train/n_updates             | 430          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123      |\n",
      "|    agent/train/value_loss            | 0.0135       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 16.9     |\n",
      "|    agent/time/fps                    | 2.4e+04  |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 9.01e+04 |\n",
      "|    agent/train/approx_kl             | 0.00784  |\n",
      "|    agent/train/clip_fraction         | 0.266    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.478   |\n",
      "|    agent/train/explained_variance    | 0.932    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0044  |\n",
      "|    agent/train/n_updates             | 440      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00865 |\n",
      "|    agent/train/value_loss            | 0.0113   |\n",
      "|    preferences/entropy               | 0.52     |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 10.5     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10.7     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.2     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.2     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 245 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1f08a7f8b34344b106379ff5407226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17.6        |\n",
      "|    agent/time/fps                    | 22699       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 92160       |\n",
      "|    agent/train/approx_kl             | 0.007844508 |\n",
      "|    agent/train/clip_fraction         | 0.266       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.478      |\n",
      "|    agent/train/explained_variance    | 0.932       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0044     |\n",
      "|    agent/train/n_updates             | 440         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00865    |\n",
      "|    agent/train/value_loss            | 0.0113      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17.6     |\n",
      "|    agent/time/fps                    | 2.27e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 9.22e+04 |\n",
      "|    agent/train/approx_kl             | 0.00714  |\n",
      "|    agent/train/clip_fraction         | 0.212    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.412   |\n",
      "|    agent/train/explained_variance    | 0.961    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0152  |\n",
      "|    agent/train/n_updates             | 450      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0107  |\n",
      "|    agent/train/value_loss            | 0.00878  |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 10.2     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.2     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.2     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 249 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5799e9ed93f34d97961bbe6dfaff9267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.8         |\n",
      "|    agent/time/fps                    | 23931        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 94208        |\n",
      "|    agent/train/approx_kl             | 0.0071424074 |\n",
      "|    agent/train/clip_fraction         | 0.212        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.412       |\n",
      "|    agent/train/explained_variance    | 0.961        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0152      |\n",
      "|    agent/train/n_updates             | 450          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0107      |\n",
      "|    agent/train/value_loss            | 0.00878      |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.8     |\n",
      "|    agent/time/fps                    | 2.39e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 9.42e+04 |\n",
      "|    agent/train/approx_kl             | 0.0054   |\n",
      "|    agent/train/clip_fraction         | 0.16     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.392   |\n",
      "|    agent/train/explained_variance    | 0.951    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.0214   |\n",
      "|    agent/train/n_updates             | 460      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00489 |\n",
      "|    agent/train/value_loss            | 0.0113   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 10.4     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10.1     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.2     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.2     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 253 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63bf1cd061e41c597222cf6fa9e43c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.4         |\n",
      "|    agent/time/fps                    | 22843        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 96256        |\n",
      "|    agent/train/approx_kl             | 0.0053970115 |\n",
      "|    agent/train/clip_fraction         | 0.16         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.392       |\n",
      "|    agent/train/explained_variance    | 0.951        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0214       |\n",
      "|    agent/train/n_updates             | 460          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00489     |\n",
      "|    agent/train/value_loss            | 0.0113       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.4     |\n",
      "|    agent/time/fps                    | 2.28e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 9.63e+04 |\n",
      "|    agent/train/approx_kl             | 0.00845  |\n",
      "|    agent/train/clip_fraction         | 0.208    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.397   |\n",
      "|    agent/train/explained_variance    | 0.958    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.00751  |\n",
      "|    agent/train/n_updates             | 470      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0075  |\n",
      "|    agent/train/value_loss            | 0.0104   |\n",
      "|    preferences/entropy               | 0.542    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 10.3     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10       |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 9.96     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 9.96     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 257 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f4696deecd4a53bec43c1cca679310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.8        |\n",
      "|    agent/time/fps                    | 22391       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 98304       |\n",
      "|    agent/train/approx_kl             | 0.008454105 |\n",
      "|    agent/train/clip_fraction         | 0.208       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.397      |\n",
      "|    agent/train/explained_variance    | 0.958       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00751     |\n",
      "|    agent/train/n_updates             | 470         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0075     |\n",
      "|    agent/train/value_loss            | 0.0104      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.8     |\n",
      "|    agent/time/fps                    | 2.24e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 9.83e+04 |\n",
      "|    agent/train/approx_kl             | 0.00702  |\n",
      "|    agent/train/clip_fraction         | 0.265    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.425   |\n",
      "|    agent/train/explained_variance    | 0.953    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0173  |\n",
      "|    agent/train/n_updates             | 480      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0118  |\n",
      "|    agent/train/value_loss            | 0.0109   |\n",
      "|    preferences/entropy               | 0.611    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 8.82     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 8.84     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 8.8      |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 8.8      |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 261 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf0011daa8a40ccbdbe093fe29f89b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.4         |\n",
      "|    agent/time/fps                    | 22988        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 100352       |\n",
      "|    agent/train/approx_kl             | 0.0070155305 |\n",
      "|    agent/train/clip_fraction         | 0.265        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.425       |\n",
      "|    agent/train/explained_variance    | 0.953        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0173      |\n",
      "|    agent/train/n_updates             | 480          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0118      |\n",
      "|    agent/train/value_loss            | 0.0109       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.4     |\n",
      "|    agent/time/fps                    | 2.3e+04  |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1e+05    |\n",
      "|    agent/train/approx_kl             | 0.0109   |\n",
      "|    agent/train/clip_fraction         | 0.28     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.441   |\n",
      "|    agent/train/explained_variance    | 0.939    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.015   |\n",
      "|    agent/train/n_updates             | 490      |\n",
      "|    agent/train/policy_gradient_loss  | -0.011   |\n",
      "|    agent/train/value_loss            | 0.0143   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.32     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 9.61     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 8.92     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 8.92     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 265 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0ad1b145be4b67a0f4e2208f707655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.4       |\n",
      "|    agent/time/fps                    | 23357      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 102400     |\n",
      "|    agent/train/approx_kl             | 0.01093177 |\n",
      "|    agent/train/clip_fraction         | 0.28       |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.441     |\n",
      "|    agent/train/explained_variance    | 0.939      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.015     |\n",
      "|    agent/train/n_updates             | 490        |\n",
      "|    agent/train/policy_gradient_loss  | -0.011     |\n",
      "|    agent/train/value_loss            | 0.0143     |\n",
      "-----------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.4     |\n",
      "|    agent/time/fps                    | 2.34e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.02e+05 |\n",
      "|    agent/train/approx_kl             | 0.00557  |\n",
      "|    agent/train/clip_fraction         | 0.255    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.479   |\n",
      "|    agent/train/explained_variance    | 0.96     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0101  |\n",
      "|    agent/train/n_updates             | 500      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0102  |\n",
      "|    agent/train/value_loss            | 0.0121   |\n",
      "|    preferences/entropy               | 0.524    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.43     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 9        |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 9.28     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 9.28     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 269 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec86e422299b4ffea063dca22b229e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.4        |\n",
      "|    agent/time/fps                    | 22420       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 104448      |\n",
      "|    agent/train/approx_kl             | 0.005570894 |\n",
      "|    agent/train/clip_fraction         | 0.255       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.479      |\n",
      "|    agent/train/explained_variance    | 0.96        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0101     |\n",
      "|    agent/train/n_updates             | 500         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0102     |\n",
      "|    agent/train/value_loss            | 0.0121      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.4     |\n",
      "|    agent/time/fps                    | 2.24e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.04e+05 |\n",
      "|    agent/train/approx_kl             | 0.00542  |\n",
      "|    agent/train/clip_fraction         | 0.238    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.487   |\n",
      "|    agent/train/explained_variance    | 0.92     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00977 |\n",
      "|    agent/train/n_updates             | 510      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00947 |\n",
      "|    agent/train/value_loss            | 0.0151   |\n",
      "|    preferences/entropy               | 0.502    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 10.6     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 9.19     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 9.05     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 9.05     |\n",
      "---------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Requested 760 transitions but only 0 in buffer. Sampling 760 additional transitions.\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 273 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a218ad687d2043c08a8afef17e24dab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.6        |\n",
      "|    agent/time/fps                    | 23449       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 106496      |\n",
      "|    agent/train/approx_kl             | 0.005423213 |\n",
      "|    agent/train/clip_fraction         | 0.238       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.487      |\n",
      "|    agent/train/explained_variance    | 0.92        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00977    |\n",
      "|    agent/train/n_updates             | 510         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00947    |\n",
      "|    agent/train/value_loss            | 0.0151      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.6     |\n",
      "|    agent/time/fps                    | 2.34e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.06e+05 |\n",
      "|    agent/train/approx_kl             | 0.00696  |\n",
      "|    agent/train/clip_fraction         | 0.256    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.517   |\n",
      "|    agent/train/explained_variance    | 0.939    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.000666 |\n",
      "|    agent/train/n_updates             | 520      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0103  |\n",
      "|    agent/train/value_loss            | 0.0169   |\n",
      "|    preferences/entropy               | 0.399    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 10.1     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10.1     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 8.99     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 8.99     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 276 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddc5342b38941c0adcfc9c626f2d507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.5         |\n",
      "|    agent/time/fps                    | 23412        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 108544       |\n",
      "|    agent/train/approx_kl             | 0.0069611417 |\n",
      "|    agent/train/clip_fraction         | 0.256        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.517       |\n",
      "|    agent/train/explained_variance    | 0.939        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.000666     |\n",
      "|    agent/train/n_updates             | 520          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0103      |\n",
      "|    agent/train/value_loss            | 0.0169       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.5     |\n",
      "|    agent/time/fps                    | 2.34e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.09e+05 |\n",
      "|    agent/train/approx_kl             | 0.00599  |\n",
      "|    agent/train/clip_fraction         | 0.281    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.543   |\n",
      "|    agent/train/explained_variance    | 0.929    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.028    |\n",
      "|    agent/train/n_updates             | 530      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0102  |\n",
      "|    agent/train/value_loss            | 0.0137   |\n",
      "|    preferences/entropy               | 0.267    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.68     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 8.96     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 9.06     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 9.06     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 279 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fe6dc003c94ea590572e9c7341c0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.9        |\n",
      "|    agent/time/fps                    | 21762       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 110592      |\n",
      "|    agent/train/approx_kl             | 0.005987121 |\n",
      "|    agent/train/clip_fraction         | 0.281       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.543      |\n",
      "|    agent/train/explained_variance    | 0.929       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.028       |\n",
      "|    agent/train/n_updates             | 530         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0102     |\n",
      "|    agent/train/value_loss            | 0.0137      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.9     |\n",
      "|    agent/time/fps                    | 2.18e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.11e+05 |\n",
      "|    agent/train/approx_kl             | 0.00592  |\n",
      "|    agent/train/clip_fraction         | 0.307    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.556   |\n",
      "|    agent/train/explained_variance    | 0.931    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0158  |\n",
      "|    agent/train/n_updates             | 540      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113  |\n",
      "|    agent/train/value_loss            | 0.0155   |\n",
      "|    preferences/entropy               | 0.0704   |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.22     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 8.84     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 8.78     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 8.78     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 282 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b91832be0b46988a949f7c3b4bfefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 22.6        |\n",
      "|    agent/time/fps                    | 24050       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 112640      |\n",
      "|    agent/train/approx_kl             | 0.005923879 |\n",
      "|    agent/train/clip_fraction         | 0.307       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.556      |\n",
      "|    agent/train/explained_variance    | 0.931       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0158     |\n",
      "|    agent/train/n_updates             | 540         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113     |\n",
      "|    agent/train/value_loss            | 0.0155      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 22.6     |\n",
      "|    agent/time/fps                    | 2.4e+04  |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.13e+05 |\n",
      "|    agent/train/approx_kl             | 0.00579  |\n",
      "|    agent/train/clip_fraction         | 0.326    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.568   |\n",
      "|    agent/train/explained_variance    | 0.941    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.000797 |\n",
      "|    agent/train/n_updates             | 550      |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115  |\n",
      "|    agent/train/value_loss            | 0.016    |\n",
      "|    preferences/entropy               | 0.258    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 8.75     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 8.96     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 8.63     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 8.63     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0198a2945ae44cadbd3c973b79337bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 24.1       |\n",
      "|    agent/time/fps                    | 20067      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 114688     |\n",
      "|    agent/train/approx_kl             | 0.00579118 |\n",
      "|    agent/train/clip_fraction         | 0.326      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.568     |\n",
      "|    agent/train/explained_variance    | 0.941      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.000797   |\n",
      "|    agent/train/n_updates             | 550        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115    |\n",
      "|    agent/train/value_loss            | 0.016      |\n",
      "-----------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 24.1     |\n",
      "|    agent/time/fps                    | 2.01e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.15e+05 |\n",
      "|    agent/train/approx_kl             | 0.00925  |\n",
      "|    agent/train/clip_fraction         | 0.313    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.555   |\n",
      "|    agent/train/explained_variance    | 0.793    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00253 |\n",
      "|    agent/train/n_updates             | 560      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00525 |\n",
      "|    agent/train/value_loss            | 0.0225   |\n",
      "|    preferences/entropy               | 0.237    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 8.97     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 8.98     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 8.73     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 8.73     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 288 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36683ee35e394a50884093f8e6a5f5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 25.6       |\n",
      "|    agent/time/fps                    | 17226      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 116736     |\n",
      "|    agent/train/approx_kl             | 0.00924789 |\n",
      "|    agent/train/clip_fraction         | 0.313      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.555     |\n",
      "|    agent/train/explained_variance    | 0.793      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.00253   |\n",
      "|    agent/train/n_updates             | 560        |\n",
      "|    agent/train/policy_gradient_loss  | -0.00525   |\n",
      "|    agent/train/value_loss            | 0.0225     |\n",
      "-----------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 25.6     |\n",
      "|    agent/time/fps                    | 1.72e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.17e+05 |\n",
      "|    agent/train/approx_kl             | 0.00696  |\n",
      "|    agent/train/clip_fraction         | 0.249    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.539   |\n",
      "|    agent/train/explained_variance    | 0.891    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.00166 |\n",
      "|    agent/train/n_updates             | 570      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00634 |\n",
      "|    agent/train/value_loss            | 0.0155   |\n",
      "|    preferences/entropy               | 0.25     |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 8.79     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 8.86     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 8.69     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 8.69     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 291 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c829dbb4b548628fa8e84fc301cb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 27.4         |\n",
      "|    agent/time/fps                    | 19973        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 118784       |\n",
      "|    agent/train/approx_kl             | 0.0069552762 |\n",
      "|    agent/train/clip_fraction         | 0.249        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.539       |\n",
      "|    agent/train/explained_variance    | 0.891        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00166     |\n",
      "|    agent/train/n_updates             | 570          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00634     |\n",
      "|    agent/train/value_loss            | 0.0155       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 27.4     |\n",
      "|    agent/time/fps                    | 2e+04    |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.19e+05 |\n",
      "|    agent/train/approx_kl             | 0.00549  |\n",
      "|    agent/train/clip_fraction         | 0.22     |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.538   |\n",
      "|    agent/train/explained_variance    | 0.92     |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0142  |\n",
      "|    agent/train/n_updates             | 580      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00535 |\n",
      "|    agent/train/value_loss            | 0.0169   |\n",
      "|    preferences/entropy               | 0.231    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.12     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 9.52     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 9.88     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 9.88     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 294 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef47fe62e8941f38ffca7f4278b43ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 29.8        |\n",
      "|    agent/time/fps                    | 21143       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 120832      |\n",
      "|    agent/train/approx_kl             | 0.005485727 |\n",
      "|    agent/train/clip_fraction         | 0.22        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.538      |\n",
      "|    agent/train/explained_variance    | 0.92        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0142     |\n",
      "|    agent/train/n_updates             | 580         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00535    |\n",
      "|    agent/train/value_loss            | 0.0169      |\n",
      "------------------------------------------------------\n",
      "----------------------------------------------------\n",
      "| mean/                                |           |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 29.8      |\n",
      "|    agent/time/fps                    | 2.11e+04  |\n",
      "|    agent/time/iterations             | 1         |\n",
      "|    agent/time/time_elapsed           | 0         |\n",
      "|    agent/time/total_timesteps        | 1.21e+05  |\n",
      "|    agent/train/approx_kl             | 0.00494   |\n",
      "|    agent/train/clip_fraction         | 0.196     |\n",
      "|    agent/train/clip_range            | 0.1       |\n",
      "|    agent/train/entropy_loss          | -0.547    |\n",
      "|    agent/train/explained_variance    | 0.938     |\n",
      "|    agent/train/learning_rate         | 0.002     |\n",
      "|    agent/train/loss                  | -0.000154 |\n",
      "|    agent/train/n_updates             | 590       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00223  |\n",
      "|    agent/train/value_loss            | 0.0187    |\n",
      "|    preferences/entropy               | 0.462     |\n",
      "|    reward/epoch-0/train/accuracy     | 0         |\n",
      "|    reward/epoch-0/train/loss         | 10.2      |\n",
      "|    reward/epoch-1/train/accuracy     | 0         |\n",
      "|    reward/epoch-1/train/loss         | 9.74      |\n",
      "|    reward/epoch-2/train/accuracy     | 0         |\n",
      "|    reward/epoch-2/train/loss         | 11.1      |\n",
      "| reward/                              |           |\n",
      "|    final/train/accuracy              | 0         |\n",
      "|    final/train/loss                  | 11.1      |\n",
      "----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 297 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab691493d1647f2a454486136a0ad52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 853 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 31.4        |\n",
      "|    agent/time/fps                    | 22207       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 122880      |\n",
      "|    agent/train/approx_kl             | 0.004942756 |\n",
      "|    agent/train/clip_fraction         | 0.196       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.547      |\n",
      "|    agent/train/explained_variance    | 0.938       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.000154   |\n",
      "|    agent/train/n_updates             | 590         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00223    |\n",
      "|    agent/train/value_loss            | 0.0187      |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 31.4     |\n",
      "|    agent/time/fps                    | 2.22e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.23e+05 |\n",
      "|    agent/train/approx_kl             | 0.00621  |\n",
      "|    agent/train/clip_fraction         | 0.206    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.54    |\n",
      "|    agent/train/explained_variance    | 0.937    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | 0.0104   |\n",
      "|    agent/train/n_updates             | 600      |\n",
      "|    agent/train/policy_gradient_loss  | -0.00466 |\n",
      "|    agent/train/value_loss            | 0.0326   |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.96     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 10.2     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 10.1     |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 10.1     |\n",
      "---------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Requested 570 transitions but only 0 in buffer. Sampling 570 additional transitions.\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 300 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a223ce60b84956ac19f576b04e2a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 833 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 33.5         |\n",
      "|    agent/time/fps                    | 23061        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 124928       |\n",
      "|    agent/train/approx_kl             | 0.0062115993 |\n",
      "|    agent/train/clip_fraction         | 0.206        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -0.54        |\n",
      "|    agent/train/explained_variance    | 0.937        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0104       |\n",
      "|    agent/train/n_updates             | 600          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00466     |\n",
      "|    agent/train/value_loss            | 0.0326       |\n",
      "-------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 33.5     |\n",
      "|    agent/time/fps                    | 2.31e+04 |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 1.25e+05 |\n",
      "|    agent/train/approx_kl             | 0.0117   |\n",
      "|    agent/train/clip_fraction         | 0.301    |\n",
      "|    agent/train/clip_range            | 0.1      |\n",
      "|    agent/train/entropy_loss          | -0.492   |\n",
      "|    agent/train/explained_variance    | 0.945    |\n",
      "|    agent/train/learning_rate         | 0.002    |\n",
      "|    agent/train/loss                  | -0.0084  |\n",
      "|    agent/train/n_updates             | 610      |\n",
      "|    agent/train/policy_gradient_loss  | -0.007   |\n",
      "|    agent/train/value_loss            | 0.0163   |\n",
      "|    preferences/entropy               | 0.231    |\n",
      "|    reward/epoch-0/train/accuracy     | 0        |\n",
      "|    reward/epoch-0/train/loss         | 9.76     |\n",
      "|    reward/epoch-1/train/accuracy     | 0        |\n",
      "|    reward/epoch-1/train/loss         | 9.65     |\n",
      "|    reward/epoch-2/train/accuracy     | 0        |\n",
      "|    reward/epoch-2/train/loss         | 9.6      |\n",
      "| reward/                              |          |\n",
      "|    final/train/accuracy              | 0        |\n",
      "|    final/train/loss                  | 9.6      |\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 9.597181844711304, 'reward_accuracy': 0.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "venv = make_vec_env(\"seals/CartPole-v0\", rng=rng)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(\n",
    "    warning_threshold=0,\n",
    "    rng=rng,\n",
    ")\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=CrossEntropyRewardLossCustom(),\n",
    "    epochs=3,\n",
    "    lr=0.001,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "\n",
    "# Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "# ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "# ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "# initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "# approximately fine-tuned to reach a reasonable level of performance.\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=2e-3,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.05,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=60,  # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=100,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=4,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")\n",
    "\n",
    "pref_comparisons.train(\n",
    "    total_timesteps=50_000,\n",
    "    total_comparisons=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96575fa7-1606-44ef-b9e6-f6526bc3f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 500 +/- 0\n"
     ]
    }
   ],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)\n",
    "\n",
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(100_000)  # Note: set to 100_000 to train a proficient expert\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 10\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std / np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4269a6-55fc-4027-a50f-79ed9f3026ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedef3d2-d230-483d-81be-5dafdeb02b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
